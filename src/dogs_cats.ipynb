{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'th'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from scipy.misc import imresize\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "K.image_dim_ordering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reducex(X, y, reduce_classes=None, reduce_percent=.2):\n",
    "#     import pdb; pdb.set_trace()\n",
    "    idxs = []\n",
    "    if reduce_classes:\n",
    "        for c in reduce_classes:\n",
    "            try:\n",
    "                idxs += list(np.where(y == c)[0])\n",
    "            except IndexError:\n",
    "                continue\n",
    "    np.random.seed(1000)\n",
    "    new_size = int(np.round(len(idxs) * reduce_percent))\n",
    "    np.random.shuffle(idxs)\n",
    "    return (X[idxs[:int(len(idxs)*reduce_percent)]], y[idxs[:int(len(idxs)*reduce_percent)]] == 3)\n",
    "\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(Xtrain, ytrain), (Xtest, ytest) = cifar10.load_data()\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "            rotation_range=40,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "Xtrain, ytrain = reducex(Xtrain, ytrain, reduce_classes=[3,5], reduce_percent=0.3)\n",
    "Xtest, ytest = reducex(Xtest, ytest, reduce_classes=[3,5], reduce_percent=0.3)\n",
    "\n",
    "yetrain = keras.utils.to_categorical(ytrain)\n",
    "yetest = keras.utils.to_categorical(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-187-15063be704f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m######## Resize cifar10 images to 3x48x48 #############\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mrsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "######## Resize cifar10 images to 3x48x48 #############\n",
    "rsize = 224\n",
    "temp = np.zeros((Xtrain.shape[0], 3, rsize, rsize))\n",
    "\n",
    "for i, row in enumerate(Xtrain):\n",
    "    temp[i] = imresize(row, size=(48,48)).transpose(2,1,0)\n",
    "\n",
    "temp2 = np.zeros((Xtrain.shape[0], 3, 48, 48))\n",
    "for i in Xtest:\n",
    "    temp2[i] = imresize(row, size=(48,48)).transpose(2,1,0)\n",
    "\n",
    "Xtrain = temp\n",
    "Xtest = temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "1 0\n",
      "1 1\n",
      "[ True]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlcAAAHdCAYAAAAw+FynAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAbG5AgAAGND/BQw60Z69oLeRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2772de80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# img = load_img('data/train/cats/cat.0.jpg')  # this is a PIL image\n",
    "# x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "x = Xtrain[8]\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "i = 0\n",
    "j = 0\n",
    "xims = []\n",
    "fig, axes = plt.subplots(2,2,figsize=(12,8))\n",
    "for batch in datagen.flow(x, batch_size=1):\n",
    "    axes[j][i].set_axis_off()\n",
    "    axes[j][i].imshow(batch.reshape(x.shape[1:]).transpose(2,1,0), interpolation='nearest')\n",
    "    if i >= 1:\n",
    "        j += 1\n",
    "        i = -1\n",
    "    if j == 2:\n",
    "        break\n",
    "    i +=1 \n",
    "    print j,i\n",
    "    \n",
    "# plt.imshow(Xtrain[0].reshape(32,32,3))\n",
    "# fig, axes1 = plt.subplots(2,2,figsize=(12,8))\n",
    "# for j in range(5):\n",
    "#     for k in range(5):\n",
    "#         i = np.random.choice(range(len(X)))\n",
    "#         axes1[j][k].set_axis_off()\n",
    "#         axes1[j][k].imshow(X[i:i+1][0], interpolation='nearest')\n",
    "print ytrain[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3L, 48L, 48L), (3L, 32L, 32L))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set(list(ytrain.reshape(-1)))\n",
    "# keras.utils.to_categorical(list(ytrain.reshape(-1)))\n",
    "t = imresize(Xtrain[0], size=(48,48)).transpose(2,1,0)\n",
    "t.shape, Xtrain[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_generator = datagen.flow(Xtrain,yetrain,\n",
    "        batch_size=batch_size)  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow(Xtest, yetest,\n",
    "        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 32, 30, 30)        896       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 32, 30, 30)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 32, 15, 15)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 13, 13)        9248      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 32, 13, 13)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 32, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 64, 4, 4)          18496     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64, 4, 4)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 64, 2, 2)          0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 66        \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 36,930.0\n",
      "Trainable params: 36,930.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten()) # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('sigmoid'))\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "7/7 [==============================] - 2s - loss: 4.0395 - acc: 0.5257 - val_loss: 0.6932 - val_acc: 0.5117\n",
      "Epoch 2/15\n",
      "7/7 [==============================] - 1s - loss: 0.7200 - acc: 0.4900 - val_loss: 0.6930 - val_acc: 0.5000\n",
      "Epoch 3/15\n",
      "7/7 [==============================] - 1s - loss: 0.6975 - acc: 0.4944 - val_loss: 0.6933 - val_acc: 0.5000\n",
      "Epoch 4/15\n",
      "7/7 [==============================] - 1s - loss: 0.6943 - acc: 0.4989 - val_loss: 0.6929 - val_acc: 0.5000\n",
      "Epoch 5/15\n",
      "7/7 [==============================] - 1s - loss: 0.6959 - acc: 0.5033 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 6/15\n",
      "7/7 [==============================] - 1s - loss: 0.6933 - acc: 0.5045 - val_loss: 0.6934 - val_acc: 0.4745\n",
      "Epoch 7/15\n",
      "7/7 [==============================] - 2s - loss: 0.7386 - acc: 0.5206 - val_loss: 0.6929 - val_acc: 0.5234\n",
      "Epoch 8/15\n",
      "7/7 [==============================] - 1s - loss: 0.6934 - acc: 0.5212 - val_loss: 0.6929 - val_acc: 0.5185\n",
      "Epoch 9/15\n",
      "7/7 [==============================] - 1s - loss: 0.6928 - acc: 0.4743 - val_loss: 0.6932 - val_acc: 0.4961\n",
      "Epoch 10/15\n",
      "7/7 [==============================] - 1s - loss: 0.7018 - acc: 0.5112 - val_loss: 0.6933 - val_acc: 0.4883\n",
      "Epoch 11/15\n",
      "7/7 [==============================] - 1s - loss: 0.6928 - acc: 0.5290 - val_loss: 0.6930 - val_acc: 0.5093\n",
      "Epoch 12/15\n",
      "7/7 [==============================] - 1s - loss: 0.6937 - acc: 0.4621 - val_loss: 0.6930 - val_acc: 0.5156\n",
      "Epoch 13/15\n",
      "7/7 [==============================] - 1s - loss: 0.7619 - acc: 0.5179 - val_loss: 0.6928 - val_acc: 0.5278\n",
      "Epoch 14/15\n",
      "7/7 [==============================] - 1s - loss: 0.8200 - acc: 0.5316 - val_loss: 0.6935 - val_acc: 0.4805\n",
      "Epoch 15/15\n",
      "7/7 [==============================] - 1s - loss: 0.6959 - acc: 0.5045 - val_loss: 0.6928 - val_acc: 0.5273\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=500 // batch_size,\n",
    "        epochs=15,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=300 // batch_size)\n",
    "import datetime\n",
    "now = str(datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256L, 32L)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.save_weights('first_try.h5')  # always save your weights after training or during training\n",
    "weights = model.weights[6:]\n",
    "weights[0].eval().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Using VGG16 Bottleneck Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n"
     ]
    }
   ],
   "source": [
    "vmodel = VGG16(include_top=True, weights='imagenet')#, input_shape=(3,48,48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vmodel.layers.pop()\n",
    "for layr in vmodel.layers:\n",
    "    layr.trainable = False\n",
    "last = vmodel.output\n",
    "# x = Flatten(last)\n",
    "x = Dense(2, activation='sigmoid')(last)\n",
    "vvmodel = keras.models.Model(vmodel.input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        (None, 3, 224, 224)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 64, 224, 224)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, 224, 224)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 64, 112, 112)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 112, 112)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, 112, 112)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 128, 56, 56)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 256, 56, 56)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 256, 28, 28)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 512, 28, 28)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 512, 14, 14)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 512, 7, 7)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 2)                 2002      \n",
      "=================================================================\n",
      "Total params: 138,359,546.0\n",
      "Trainable params: 4,099,002.0\n",
      "Non-trainable params: 134,260,544.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# vmodel.add(Dense(2))\n",
    "# vmodel.add(Activation('sigmoid'))\n",
    "vvmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow(Xtrain,\n",
    "        batch_size=batch_size, shuffle=True)  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow(Xtest,\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NumpyArrayIterator' object has no attribute 'flow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-172-bb9ea781a7b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myetrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mval_batches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_datagen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myetest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NumpyArrayIterator' object has no attribute 'flow'"
     ]
    }
   ],
   "source": [
    "# train_batches = train_generator.flow(Xtrain, yetrain, batch_size=batch_size, shuffle=True)\n",
    "# val_batches = test_datagen.flow(Xtest, yetest, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = RMSprop(lr=0.01)\n",
    "vvmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_model(model, batches, val_batches, epochs=1):\n",
    "    model.fit_generator(batches, steps_per_epoch=1500 // batch_size, epochs=epochs, \n",
    "                        validation_steps=700 // batch_size,\n",
    "                        validation_data=val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: expected input_17 to have shape (None, 3, 224, 224) but got array with shape (100L, 3L, 48L, 48L)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-186-38aae1b22819>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvvmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myetrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\apps\\python2.7\\python-2.7.10.amd64\\lib\\site-packages\\keras-2.0.2-py2.7.egg\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1403\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1404\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1405\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1406\u001b[0m         \u001b[1;31m# prepare validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1407\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\python2.7\\python-2.7.10.amd64\\lib\\site-packages\\keras-2.0.2-py2.7.egg\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1293\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1295\u001b[1;33m                                     exception_prefix='model input')\n\u001b[0m\u001b[0;32m   1296\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[0;32m   1297\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\python2.7\\python-2.7.10.amd64\\lib\\site-packages\\keras-2.0.2-py2.7.egg\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    131\u001b[0m                             \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m                             \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m                             str(array.shape))\n\u001b[0m\u001b[0;32m    134\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model input: expected input_17 to have shape (None, 3, 224, 224) but got array with shape (100L, 3L, 48L, 48L)"
     ]
    }
   ],
   "source": [
    "vvmodel.fit(Xtrain[0:100], yetrain[0:100], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: [[[[   7.    9.   10. ...,  132.  130.  130.]\n   [   7.    9.    9. ...,  130.  114.  114.]\n   [   7.    7.    9. ...,  114.   95.   95.]\n   ..., \n   [ 228.  237.  237. ...,    0.   25.   25.]\n   [ 237.  243.  243. ...,   23.   48.   50.]\n   [ 243.  243.  236. ...,   46.   46.   48.]]\n\n  [[   5.    7.    8. ...,  127.  124.  124.]\n   [   5.    7.    7. ...,  124.  111.  111.]\n   [   5.    5.    7. ...,  111.   92.   92.]\n   ..., \n   [ 218.  230.  230. ...,    0.   26.   26.]\n   [ 230.  239.  239. ...,   24.   49.   50.]\n   [ 239.  239.  231. ...,   47.   47.   49.]]\n\n  [[   6.    8.    9. ...,   91.   90.   90.]\n   [   6.    8.    8. ...,   90.   77.   77.]\n   [   6.    6.    8. ...,   77.   60.   60.]\n   ..., \n   [ 192.  207.  207. ...,    0.   18.   18.]\n   [ 207.  217.  217. ...,   17.   34.   35.]\n   [ 217.  217.  206. ...,   33.   33.   34.]]]\n\n\n [[[  15.   15.   15. ...,   63.  117.  146.]\n   [  15.   15.   15. ...,  120.  156.  171.]\n   [  15.   15.   15. ...,  127.  148.  159.]\n   ..., \n   [ 176.  176.  176. ...,  116.  119.  121.]\n   [ 176.  176.  176. ...,   89.   96.  121.]\n   [ 176.  176.  176. ...,   66.   76.  101.]]\n\n  [[  18.   23.   23. ...,   90.  150.  182.]\n   [  18.   18.   18. ...,  130.  168.  183.]\n   [  15.   15.   15. ...,  113.  127.  135.]\n   ..., \n   [  85.   85.   85. ...,   45.   45.   46.]\n   [  85.   85.   85. ...,   28.   30.   46.]\n   [  85.   85.   85. ...,   14.   19.   32.]]\n\n  [[   7.   16.   16. ...,   83.  143.  176.]\n   [   7.    7.    7. ...,  123.  159.  174.]\n   [   5.    5.    5. ...,   98.  111.  117.]\n   ..., \n   [  38.   38.   38. ...,   21.   21.   21.]\n   [  38.   38.   38. ...,   14.   15.   21.]\n   [  38.   38.   38. ...,    8.   11.   15.]]]\n\n\n [[[ 189.  189.  188. ...,  211.  211.  213.]\n   [ 188.  188.  189. ...,  213.  213.  215.]\n   [ 187.  188.  193. ...,  213.  215.  216.]\n   ..., \n   [ 170.  170.  167. ...,  193.  195.  197.]\n   [ 167.  167.  145. ...,  193.  193.  195.]\n   [ 167.  145.  121. ...,  190.  193.  195.]]\n\n  [[ 190.  190.  189. ...,  210.  210.  212.]\n   [ 190.  190.  191. ...,  212.  212.  214.]\n   [ 189.  190.  196. ...,  212.  214.  215.]\n   ..., \n   [ 182.  182.  179. ...,  193.  194.  196.]\n   [ 179.  179.  156. ...,  193.  193.  194.]\n   [ 179.  156.  133. ...,  190.  193.  194.]]\n\n  [[ 194.  194.  194. ...,  215.  215.  217.]\n   [ 193.  193.  196. ...,  217.  217.  219.]\n   [ 193.  194.  202. ...,  217.  219.  221.]\n   ..., \n   [ 189.  189.  187. ...,  200.  202.  204.]\n   [ 187.  187.  162. ...,  200.  200.  202.]\n   [ 187.  162.  138. ...,  197.  200.  202.]]]\n\n\n ..., \n [[[  58.   96.  111. ...,  180.  180.  188.]\n   [  58.   96.  111. ...,  180.  188.  190.]\n   [  34.   58.   96. ...,  188.  190.  190.]\n   ..., \n   [   7.    1.    1. ...,   72.   69.   62.]\n   [   1.    1.    1. ...,   73.   72.   69.]\n   [   1.    1.    2. ...,   73.   72.   69.]]\n\n  [[  58.   97.  114. ...,  155.  155.  159.]\n   [  58.   97.  114. ...,  155.  159.  156.]\n   [  33.   58.   97. ...,  159.  156.  156.]\n   ..., \n   [   8.    2.    2. ...,   68.   64.   57.]\n   [   2.    2.    0. ...,   70.   68.   64.]\n   [   0.    0.    1. ...,   70.   68.   64.]]\n\n  [[  64.  105.  123. ...,  132.  132.  134.]\n   [  64.  105.  123. ...,  132.  134.  132.]\n   [  37.   64.  105. ...,  134.  132.  132.]\n   ..., \n   [   8.    2.    2. ...,   72.   68.   59.]\n   [   2.    2.    0. ...,   74.   72.   68.]\n   [   0.    0.    0. ...,   74.   72.   68.]]]\n\n\n [[[ 241.  241.  226. ...,  217.  217.  217.]\n   [ 234.  234.  215. ...,  192.  192.  217.]\n   [ 236.  236.  192. ...,  166.  166.  192.]\n   ..., \n   [  78.   79.   79. ...,  123.  124.  124.]\n   [  79.   79.   79. ...,  126.  123.  123.]\n   [  79.   79.   79. ...,  130.  126.  126.]]\n\n  [[  75.   75.   76. ...,  194.  194.  194.]\n   [  61.   61.   62. ...,  158.  158.  194.]\n   [  75.   75.   65. ...,  127.  127.  158.]\n   ..., \n   [  55.   55.   56. ...,   94.   96.   96.]\n   [  55.   55.   56. ...,   97.   94.   94.]\n   [  55.   55.   56. ...,  100.   97.   97.]]\n\n  [[  79.   79.   83. ...,  191.  191.  191.]\n   [  59.   59.   65. ...,  156.  156.  191.]\n   [  69.   69.   72. ...,  125.  125.  156.]\n   ..., \n   [  55.   55.   53. ...,  104.  106.  106.]\n   [  55.   53.   53. ...,  108.  104.  104.]\n   [  55.   53.   53. ...,  112.  108.  108.]]]\n\n\n [[[ 175.  173.  171. ...,   87.   92.  100.]\n   [ 176.  175.  173. ...,   87.   92.  100.]\n   [ 187.  176.  175. ...,   92.  100.  112.]\n   ..., \n   [ 191.  186.  184. ...,  143.  143.  150.]\n   [ 186.  184.  184. ...,  144.  144.  143.]\n   [ 186.  184.  184. ...,  147.  147.  144.]]\n\n  [[ 166.  163.  161. ...,   61.   65.   72.]\n   [ 168.  166.  163. ...,   61.   65.   72.]\n   [ 179.  168.  166. ...,   65.   72.   83.]\n   ..., \n   [ 185.  180.  177. ...,  130.  130.  136.]\n   [ 180.  177.  177. ...,  130.  130.  130.]\n   [ 180.  177.  174. ...,  133.  133.  130.]]\n\n  [[ 152.  147.  143. ...,   45.   48.   53.]\n   [ 154.  152.  147. ...,   45.   48.   53.]\n   [ 167.  154.  152. ...,   48.   53.   62.]\n   ..., \n   [ 171.  165.  160. ...,  108.  108.  114.]\n   [ 165.  160.  160. ...,  105.  105.  108.]\n   [ 165.  160.  158. ...,  106.  106.  105.]]]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-184-e1285cb08955>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvvmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# model.fit_generator(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#         train_generator,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#         steps_per_epoch=500 // batch_size,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-176-45628973f34d>\u001b[0m in \u001b[0;36mfit_model\u001b[1;34m(model, batches, val_batches, epochs)\u001b[0m\n\u001b[0;32m      2\u001b[0m     model.fit_generator(batches, steps_per_epoch=1500 // batch_size, epochs=epochs, \n\u001b[0;32m      3\u001b[0m                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m700\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                         validation_data=val_batches)\n\u001b[0m",
      "\u001b[1;32mC:\\apps\\python2.7\\python-2.7.10.amd64\\lib\\site-packages\\keras-2.0.2-py2.7.egg\\keras\\legacy\\interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\apps\\python2.7\\python-2.7.10.amd64\\lib\\site-packages\\keras-2.0.2-py2.7.egg\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1859\u001b[0m                                          \u001b[1;34m'a tuple `(x, y, sample_weight)` '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1860\u001b[0m                                          \u001b[1;34m'or `(x, y)`. Found: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1861\u001b[1;33m                                          str(generator_output))\n\u001b[0m\u001b[0;32m   1862\u001b[0m                     \u001b[1;31m# build batch logs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1863\u001b[0m                     \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: [[[[   7.    9.   10. ...,  132.  130.  130.]\n   [   7.    9.    9. ...,  130.  114.  114.]\n   [   7.    7.    9. ...,  114.   95.   95.]\n   ..., \n   [ 228.  237.  237. ...,    0.   25.   25.]\n   [ 237.  243.  243. ...,   23.   48.   50.]\n   [ 243.  243.  236. ...,   46.   46.   48.]]\n\n  [[   5.    7.    8. ...,  127.  124.  124.]\n   [   5.    7.    7. ...,  124.  111.  111.]\n   [   5.    5.    7. ...,  111.   92.   92.]\n   ..., \n   [ 218.  230.  230. ...,    0.   26.   26.]\n   [ 230.  239.  239. ...,   24.   49.   50.]\n   [ 239.  239.  231. ...,   47.   47.   49.]]\n\n  [[   6.    8.    9. ...,   91.   90.   90.]\n   [   6.    8.    8. ...,   90.   77.   77.]\n   [   6.    6.    8. ...,   77.   60.   60.]\n   ..., \n   [ 192.  207.  207. ...,    0.   18.   18.]\n   [ 207.  217.  217. ...,   17.   34.   35.]\n   [ 217.  217.  206. ...,   33.   33.   34.]]]\n\n\n [[[  15.   15.   15. ...,   63.  117.  146.]\n   [  15.   15.   15. ...,  120.  156.  171.]\n   [  15.   15.   15. ...,  127.  148.  159.]\n   ..., \n   [ 176.  176.  176. ...,  116.  119.  121.]\n   [ 176.  176.  176. ...,   89.   96.  121.]\n   [ 176.  176.  176. ...,   66.   76.  101.]]\n\n  [[  18.   23.   23. ...,   90.  150.  182.]\n   [  18.   18.   18. ...,  130.  168.  183.]\n   [  15.   15.   15. ...,  113.  127.  135.]\n   ..., \n   [  85.   85.   85. ...,   45.   45.   46.]\n   [  85.   85.   85. ...,   28.   30.   46.]\n   [  85.   85.   85. ...,   14.   19.   32.]]\n\n  [[   7.   16.   16. ...,   83.  143.  176.]\n   [   7.    7.    7. ...,  123.  159.  174.]\n   [   5.    5.    5. ...,   98.  111.  117.]\n   ..., \n   [  38.   38.   38. ...,   21.   21.   21.]\n   [  38.   38.   38. ...,   14.   15.   21.]\n   [  38.   38.   38. ...,    8.   11.   15.]]]\n\n\n [[[ 189.  189.  188. ...,  211.  211.  213.]\n   [ 188.  188.  189. ...,  213.  213.  215.]\n   [ 187.  188.  193. ...,  213.  215.  216.]\n   ..., \n   [ 170.  170.  167. ...,  193.  195.  197.]\n   [ 167.  167.  145. ...,  193.  193.  195.]\n   [ 167.  145.  121. ...,  190.  193.  195.]]\n\n  [[ 190.  190.  189. ...,  210.  210.  212.]\n   [ 190.  190.  191. ...,  212.  212.  214.]\n   [ 189.  190.  196. ...,  212.  214.  215.]\n   ..., \n   [ 182.  182.  179. ...,  193.  194.  196.]\n   [ 179.  179.  156. ...,  193.  193.  194.]\n   [ 179.  156.  133. ...,  190.  193.  194.]]\n\n  [[ 194.  194.  194. ...,  215.  215.  217.]\n   [ 193.  193.  196. ...,  217.  217.  219.]\n   [ 193.  194.  202. ...,  217.  219.  221.]\n   ..., \n   [ 189.  189.  187. ...,  200.  202.  204.]\n   [ 187.  187.  162. ...,  200.  200.  202.]\n   [ 187.  162.  138. ...,  197.  200.  202.]]]\n\n\n ..., \n [[[  58.   96.  111. ...,  180.  180.  188.]\n   [  58.   96.  111. ...,  180.  188.  190.]\n   [  34.   58.   96. ...,  188.  190.  190.]\n   ..., \n   [   7.    1.    1. ...,   72.   69.   62.]\n   [   1.    1.    1. ...,   73.   72.   69.]\n   [   1.    1.    2. ...,   73.   72.   69.]]\n\n  [[  58.   97.  114. ...,  155.  155.  159.]\n   [  58.   97.  114. ...,  155.  159.  156.]\n   [  33.   58.   97. ...,  159.  156.  156.]\n   ..., \n   [   8.    2.    2. ...,   68.   64.   57.]\n   [   2.    2.    0. ...,   70.   68.   64.]\n   [   0.    0.    1. ...,   70.   68.   64.]]\n\n  [[  64.  105.  123. ...,  132.  132.  134.]\n   [  64.  105.  123. ...,  132.  134.  132.]\n   [  37.   64.  105. ...,  134.  132.  132.]\n   ..., \n   [   8.    2.    2. ...,   72.   68.   59.]\n   [   2.    2.    0. ...,   74.   72.   68.]\n   [   0.    0.    0. ...,   74.   72.   68.]]]\n\n\n [[[ 241.  241.  226. ...,  217.  217.  217.]\n   [ 234.  234.  215. ...,  192.  192.  217.]\n   [ 236.  236.  192. ...,  166.  166.  192.]\n   ..., \n   [  78.   79.   79. ...,  123.  124.  124.]\n   [  79.   79.   79. ...,  126.  123.  123.]\n   [  79.   79.   79. ...,  130.  126.  126.]]\n\n  [[  75.   75.   76. ...,  194.  194.  194.]\n   [  61.   61.   62. ...,  158.  158.  194.]\n   [  75.   75.   65. ...,  127.  127.  158.]\n   ..., \n   [  55.   55.   56. ...,   94.   96.   96.]\n   [  55.   55.   56. ...,   97.   94.   94.]\n   [  55.   55.   56. ...,  100.   97.   97.]]\n\n  [[  79.   79.   83. ...,  191.  191.  191.]\n   [  59.   59.   65. ...,  156.  156.  191.]\n   [  69.   69.   72. ...,  125.  125.  156.]\n   ..., \n   [  55.   55.   53. ...,  104.  106.  106.]\n   [  55.   53.   53. ...,  108.  104.  104.]\n   [  55.   53.   53. ...,  112.  108.  108.]]]\n\n\n [[[ 175.  173.  171. ...,   87.   92.  100.]\n   [ 176.  175.  173. ...,   87.   92.  100.]\n   [ 187.  176.  175. ...,   92.  100.  112.]\n   ..., \n   [ 191.  186.  184. ...,  143.  143.  150.]\n   [ 186.  184.  184. ...,  144.  144.  143.]\n   [ 186.  184.  184. ...,  147.  147.  144.]]\n\n  [[ 166.  163.  161. ...,   61.   65.   72.]\n   [ 168.  166.  163. ...,   61.   65.   72.]\n   [ 179.  168.  166. ...,   65.   72.   83.]\n   ..., \n   [ 185.  180.  177. ...,  130.  130.  136.]\n   [ 180.  177.  177. ...,  130.  130.  130.]\n   [ 180.  177.  174. ...,  133.  133.  130.]]\n\n  [[ 152.  147.  143. ...,   45.   48.   53.]\n   [ 154.  152.  147. ...,   45.   48.   53.]\n   [ 167.  154.  152. ...,   48.   53.   62.]\n   ..., \n   [ 171.  165.  160. ...,  108.  108.  114.]\n   [ 165.  160.  160. ...,  105.  105.  108.]\n   [ 165.  160.  158. ...,  106.  106.  105.]]]]"
     ]
    }
   ],
   "source": [
    "fit_model(vvmodel, train_generator, validation_generator, epochs=2)\n",
    "\n",
    "# model.fit_generator(\n",
    "#         train_generator,\n",
    "#         steps_per_epoch=500 // batch_size,\n",
    "#         epochs=15,\n",
    "#         validation_data=validation_generator,\n",
    "#         validation_steps=300 // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bottleneck_features_train = vmodel.predict_generator(\n",
    "        train_generator, 500)\n",
    "np.save(open('bottleneck_features_train.npy', 'w'),\n",
    "            bottleneck_features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=(256,32),weights=np.zeros((256,32))))\n",
    "top_model.add(Dense(32))\n",
    "top_model.add(Activation('relu'))\n",
    "top_model.add(Dropout(0.5))\n",
    "top_model.add(Dense(2))\n",
    "top_model.add(Activation('sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# top_model.set_weights(weights)\n",
    "top_model.layers[0].set_weights(weights[0].eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_model_weights_path = 'first_try.h5'\n",
    "top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "# add the model on top of the convolutional base\n",
    "vmodel.add(top_model)\n",
    "\n",
    "# set the first 25 layers (up to the last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model.layers[:25]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "vmodel.compile(loss='binary_crossentropy',\n",
    "              optimizer=SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# prepare data augmentation configuration\n",
    "# train_datagen = ImageDataGenerator(\n",
    "#     rescale=1. / 255,\n",
    "#     shear_range=0.2,\n",
    "#     zoom_range=0.2,\n",
    "#     horizontal_flip=True)\n",
    "\n",
    "# test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "# train_generator = train_datagen.flow_from_directory(\n",
    "#     train_data_dir,\n",
    "#     target_size=(img_height, img_width),\n",
    "#     batch_size=batch_size,\n",
    "#     class_mode='binary')\n",
    "\n",
    "# validation_generator = test_datagen.flow_from_directory(\n",
    "#     validation_data_dir,\n",
    "#     target_size=(img_height, img_width),\n",
    "#     batch_size=batch_size,\n",
    "#     class_mode='binary')\n",
    "\n",
    "# fine-tune the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "vmodel.fit_generator(\n",
    "    train_generator,\n",
    "    samples_per_epoch=nb_train_samples,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    nb_val_samples=nb_validation_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
